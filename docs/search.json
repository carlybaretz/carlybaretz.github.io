[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Carly Baretz",
    "section": "",
    "text": "tidy: Tidy Tuesday Analysis\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nhistorical_spending &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-13/historical_spending.csv')\n\nRows: 13 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): Year, PercentCelebrating, PerPerson, Candy, Flowers, Jewelry, Gree...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ngifts_age &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-13/gifts_age.csv')\n\nRows: 6 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Age\ndbl (8): SpendingCelebrating, Candy, Flowers, Jewelry, GreetingCards, Evenin...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ngifts_gender &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-13/gifts_gender.csv')\n\nRows: 2 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Gender\ndbl (8): SpendingCelebrating, Candy, Flowers, Jewelry, GreetingCards, Evenin...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nggplot(data=gifts_age, aes(x= Age, y = Flowers))+ \n  geom_point()+\n  ggtitle(\"Valentines Day Buying Habits and Age\")\n\n\n\n\n\n\n\n\n\nbabynames &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-03-22/babynames.csv')\n\nRows: 1924665 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): sex, name\ndbl (3): year, n, prop\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbabynames &lt;- babynames %&gt;% \n filter(n &gt; 80000)\n\nggplot(babynames, aes(x= \"\", y = prop, fill= name))+ \n  geom_col()+\n  coord_polar(theta=\"y\")+\n  ggtitle(\"2022 Baby Names for Names With More Than 80,000 Babies\")"
  },
  {
    "objectID": "babynames.html",
    "href": "babynames.html",
    "title": "Baby Names Data Visualization",
    "section": "",
    "text": "Names carry cultural and historical significance, reflecting societal trends and personal preferences. This analysis examines the popularity of baby names in the United States using data sourced from the Social Security Administration (SSA), as provided by the Tidy Tuesday project (March 2022). I’ll explore the most popular names with over 80,000 instances recorded in a year and visualize their proportions.\n\nlibrary(tidyverse) # Load necessary libraries\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) # Suppress warnings and messages for a cleaner website\n\n\n# Load the dataset \nbabynames &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-03-22/babynames.csv', show_col_types = FALSE)\n\nTo start I want to begin by exploring some of the key variables in the dataset.\n\nhead(babynames)\n\n# A tibble: 6 × 5\n   year sex   name          n   prop\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1  1880 F     Mary       7065 0.0724\n2  1880 F     Anna       2604 0.0267\n3  1880 F     Emma       2003 0.0205\n4  1880 F     Elizabeth  1939 0.0199\n5  1880 F     Minnie     1746 0.0179\n6  1880 F     Margaret   1578 0.0162\n\n# Summarize key metrics\nbabynames |&gt;\n  summarize(\n    total_records = n(),\n    unique_names = n_distinct(name),\n    earliest_year = min(year),\n    latest_year = max(year)\n  )\n\n# A tibble: 1 × 4\n  total_records unique_names earliest_year latest_year\n          &lt;int&gt;        &lt;int&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1       1924665        97310          1880        2017\n\n\nThen I filtered he dataset for names with over 80,000 occurrences and visualized their proportions.\n\n# Filter for names with more than 80,000 instances\nbabynames &lt;- babynames |&gt;\n  filter(n &gt; 80000)\n\n# Create a polar bar chart\nggplot(babynames, aes(x = \"\", y = prop, fill = name)) +\n  geom_col() +\n  coord_polar(theta = \"y\") +\n  labs(\n    title = \"Proportion of Popular Baby Names (Over 80,000 Instances)\",\n    x = NULL,\n    y = \"Proportion\",\n    fill = \"Name\",\n    caption = \"Source: SSA via Tidy Tuesday (March 2022)\"\n  ) \n\n\n\n\n\n\n\n\nThis analysis explored trends in U.S. baby names using data from the Social Security Administration (SSA). It focused on identifying the most popular names, specifically those with over 80,000 occurrences. Visualizations highlighted the dominance of specific names. For instance, a polar bar chart showed the proportional popularity of these names.\nThe dataset originates from the Social Security Administration (SSA), which tracks baby names through Social Security card applications. The SSA publishes this data annually to document naming trends and preferences in the U.S. This dataset was shared by the Tidy Tuesday project on March 22, 2022, as part of an initiative to encourage data analysis and visualization."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Carly Baretz",
    "section": "",
    "text": "I’m Carly Baretz. I’m from New York City and I’m a sophomore at Pitzer College studying Human Centered Design and Data Science!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "valentines.html",
    "href": "valentines.html",
    "title": "Valentines Day Data Visualization",
    "section": "",
    "text": "Valentine’s Day is a widely celebrated occasion known for its associated spending habits on gifts like flowers, candy, and jewelry. This analysis explores data from Tidy Tuesday (February 2024), which provides insights into Valentine’s Day consumer behaviors across different demographics in the United States. I will investigate how spending varies by age and gender.\n\nlibrary(tidyverse) # Load necessary libraries\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) # Suppress warnings and messages\n\nThe dataset includes three main tables: historical_spending, gifts_age, and gifts_gender.\n\n# Load the data \nhistorical_spending &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-13/historical_spending.csv')\ngifts_age &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-13/gifts_age.csv')\ngifts_gender &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-13/gifts_gender.csv')\n\nThe gifts_age dataset breaks down spending by age group and gift type. Below is a plot showing flower spending across different age groups.\n\n#| eval: true \n# Plot spending on flowers by age\ngifts_age |&gt;\n  ggplot(aes(x = Age, y = Flowers)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", color = \"green\") +\n  labs(\n    title = \"Valentine's Day Flower Spending by Age\",\n    x = \"Age\",\n    y = \"Spending on Flowers (in dollars)\",\n    caption = \"Source: Tidy Tuesday Valentine's Data (2024)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe gifts_gender dataset provides insights into how men and women differ in their spending on Valentine’s Day gifts.\n\n#| eval: true \n# Plot spending on flowers by gender\ngifts_gender |&gt;\n  ggplot(aes(x = Gender, y = Flowers, fill = Gender)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Valentine's Day Flower Spending by Gender\",\n    x = \"Gender\",\n    y = \"Average Spending on Flowers (in dollars)\",\n    caption = \"Source: Tidy Tuesday Valentine's Data (2024)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe datasets used in this analysis were sourced from the Tidy Tuesday Valentine’s Day 2024 project. The original data comes from 7,728 U.S. adult consumers, and likely comes from consumer surveys and industry reports related to Valentine’s Day shopping habits. Tidy Tuesday curates these datasets for educational purposes, and they may be cleaned or simplified versions of the original data."
  },
  {
    "objectID": "Project2.html",
    "href": "Project2.html",
    "title": "Netflix Data Analysis",
    "section": "",
    "text": "Netflix is the biggest streaming service in the world, offering a wide range of content in various genres. This analysis examines Netflix’s catalog using data from the Tidy Tuesday project (April 20, 2021). I will explore the most prevalent genres on Netflix, patterns in the word count of titles and their use of common terms like “Love,” “Story,” or “Adventure,”and trends in the growth of Netflix’s catalog over time.\n\nnetflix_titles &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv')\n\n# Calculate the number of genres listed for each title\nnetflix_titles &lt;- netflix_titles |&gt;\n  mutate(genre_count = str_count(listed_in, \",\") + 1)\n\n# Separate genres into rows and calculate counts\ngenres &lt;- netflix_titles |&gt;\n  separate_rows(listed_in, sep = \",\")|&gt;\n  group_by(listed_in) |&gt;\n  summarise(count = n()) |&gt;\n  arrange(desc(count))\n\n# Plot the top 10 genres by count\nggplot(genres[1:10,], aes(x = reorder(listed_in, -count), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"blue\") +\n  labs(title = \"Top 10 Genres by Number of Titles\",\n       x = \"Genre\", y = \"Count\") +\n  theme_minimal() +\n  coord_flip()\n\n\n\n\n\n\n\n\nThe bar graph above shows the top 10 genres on Netflix by amount of titles. You can see that Netflix’s largest genre by which is has the most titles is international movies, with Netflix having upwards of 2,500 international movies.\n\nnetflix_titles &lt;- netflix_titles |&gt;\n  mutate(\n    title_word_count = str_count(title, \"\\\\b\\\\w+\\\\b\"),\n    contains_common_word = str_detect(title, \"\\\\b(Love|Story|Adventure)\\\\b\")\n  )\n\ntitle_word_distribution &lt;- netflix_titles |&gt;\n  group_by(title_word_count, contains_common_word) |&gt;\n  summarise(count = n()) |&gt;\n  arrange(desc(count))\n\n\nggplot(title_word_distribution, aes(x = factor(title_word_count), y = count, fill = contains_common_word)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Distribution of Netflix Titles by Word Count and Common Words (Love, Story, Adventure)\",\n       x = \"Number of Words in Title\", y = \"Count of Titles\",\n       fill = \"Contains 'Love', 'Story', or 'Adventure'\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis graph shows the distribution of number of words in titles on Netflix, showing that most of the titles on Netflix are 2 words long. Additionally, the graph shows the prevalence of titles with 3 common words: love, story, and adventure. The graph shows that most of the titles do not contain “love,” “story,” or “adventure,” but it shows that if the words do appear, they show up in titles ranging from 1 word long to 13 words long.\n\nnetflix_titles &lt;- netflix_titles |&gt;\n  mutate(year_added = str_extract(date_added, \"\\\\d{4}\"))\n\nnetflix_titles &lt;- netflix_titles |&gt;\n  filter(!is.na(year_added))\n\ntitles_by_year &lt;- netflix_titles |&gt;\n  group_by(year_added) |&gt;\n  summarise(count = n()) |&gt;\n  arrange(year_added)\n\nggplot(titles_by_year, aes(x = year_added, y = count)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Number of Netflix Titles Added by Year\",\n       x = \"Year\", y = \"Number of Titles\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis graph shows the number of titles of shows or movies added to Netflix each year. The first titles were added to Netflix in 2008 and in 2019 the highest number of titles were added to Netflix.\nTo look for patterns in the dataset to then visualize, I started by identifying how many Netflix titles have a year in the title name, have a colon in the title, or start with “The.”\n\n# Add columns based on regular expression patterns\nnetflix_titles &lt;- netflix_titles |&gt;\n  mutate(\n    contains_year = str_detect(title, \"\\\\b(19|20)\\\\d{2}\\\\b\"),  # Detect years like 1999 or 2021\n    contains_colon = str_detect(title, \":\"),                  # Detect titles with colons\n    starts_with_the = str_detect(title, \"^The\\\\b\")            # Detect titles starting with \"The\"\n  )\n\n# Summarize the counts of titles matching each pattern\npattern_summary &lt;- netflix_titles |&gt;\n  summarize(\n    with_year = sum(contains_year, na.rm = TRUE),\n    with_colon = sum(contains_colon, na.rm = TRUE),\n    starts_with_the = sum(starts_with_the, na.rm = TRUE)\n  )\n\npattern_summary\n\n# A tibble: 1 × 3\n  with_year with_colon starts_with_the\n      &lt;int&gt;      &lt;int&gt;           &lt;int&gt;\n1        33       1262             981\n\n\nNext I made a bar plot to visualize the amount of Netflix titles with the given pattern.\n\n#| echo: false \n#| eval: true \n# Prepare data for visualization\npattern_data &lt;- pattern_summary |&gt;\n  pivot_longer(cols = everything(), names_to = \"Pattern\", values_to = \"Count\") |&gt;\n  mutate(Pattern = recode(Pattern,\n                          \"with_year\" = \"Contains Year\",\n                          \"with_colon\" = \"Contains Colon\",\n                          \"starts_with_the\" = \"Starts with 'The'\"))\n\n# Plot the counts of each pattern\nggplot(pattern_data, aes(x = reorder(Pattern, Count), y = Count, fill = Pattern)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(\n    title = \"Frequency of Specific Patterns in Netflix Titles\",\n    x = \"Pattern\",\n    y = \"Count\",\n    fill = \"Pattern\",\n    caption = \"Source: Netflix Movies and TV Shows Dataset (Tidy Tuesday)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFinally, I made a bar plot to visualize the distribution of years mentioned in Netflix titles. Below is the spread of Netflix titles containing years, and the amount of titles with names a year.\n\n# Extract titles with years and summarize their counts\ntitles_with_years &lt;- netflix_titles |&gt;\n  filter(contains_year) |&gt;\n  mutate(year_in_title = str_extract(title, \"\\\\b(19|20)\\\\d{2}\\\\b\")) |&gt;\n  group_by(year_in_title) |&gt;\n  summarize(count = n()) |&gt;\n  arrange(desc(count))\n\n# Plot the distribution of years in Netflix titles\nggplot(titles_with_years, aes(x = year_in_title, y = count)) +\n  geom_bar(stat = \"identity\", fill = \"orange\") +\n  labs(\n    title = \"Distribution of Years Mentioned in Netflix Titles\",\n    x = \"Year in Title\",\n    y = \"Count\",\n    caption = \"Source: Netflix Movies and TV Shows Dataset (Tidy Tuesday)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\nThe dataset was sourced from the Tidy Tuesday project (April 20, 2021) and is originally from Kaggle’s Netflix Movies and TV Shows dataset. The dataset aggregates publicly available information on Netflix’s catalog, including title, genre, date added, director, cast, and release year."
  },
  {
    "objectID": "ds2-hw3-carlybaretz/ds2-hw3-wrangle.html",
    "href": "ds2-hw3-carlybaretz/ds2-hw3-wrangle.html",
    "title": "DS002R - HW 3 - Wrangling",
    "section": "",
    "text": "Back to the Midwest!\nIn this part you will revisit and build on some of your findings from HW 2, where you explored the midwest data frame. Remember that this data frame is bundled with the ggplot2 package and is automatically loaded when you load the tidyverse package. As a refresher, the data contains demographic characteristics of counties in the Midwest region of the United States. You can read documentation for the data set, including variable definitions by typing ?midwest in the Console or searching for midwest in the Help pane.\n\n\n\n\n\n\nNote\n\n\n\nThe data in the midwest data frame are from the 2000 census, so the information you compute below is likely slightly different from the values today.\n\n\n\n\nCalculate the number of counties in each state and display your results in descending order of number of counties. Which state has the highest number of counties, and how many? Which state has the lowest number, and how many?\n\n\n\nWithin a state, two counties can’t have the same name, but across states county names can be shared. A friend says “Look, there is a county called XYZ in EVERY state in this dataset!” In a single pipeline, discover all counties that could fill in the value of XYZ. Your output should be a data frame with two columns: county and the number of times they appear in the data.\n\n\n\n\n\n\nTip\n\n\n\nYou will want to use the filter() function in your answer, which requires a logical condition to describe what you want to filter for. For example filter(x &gt; 2) means filter for values of x greater than 2, filter(y &lt;= 3) means filter for values of y less than or equal to 3.\n\n\n\noperator\ndefinition\n\n\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to\n\n\nx & y\nx AND y\n\n\nx | y\nx OR y\n\n\nis.na(x)\ntest if x is NA\n\n\n!is.na(x)\ntest if x is not NA\n\n\nx %in% y\ntest if x is in y\n\n\n!(x %in% y)\ntest if x is not in y\n\n\n!x\nnot x\n\n\n\nThe table above is a summary of logical operators and how to articulate them in R.\n\n\n\n\n\nConsider the following box plot of population densities where you were can see some counties have extreme values.\n\n\n\n\n\n\n\n\n\nIdentify the counties described in each part:\n\nThe counties with a population density higher than 25,000. Your code must use the filter() function.\nThe county with the highest population density. Your code must use the max() function.\n\n\n\n\n\n\n\nNote\n\n\n\nAnswer using a single data wrangling pipeline for each part. Your response should be a data frame with five columns: county name, state name, population density, total population, and area, in this order. If your response has multiple rows, the data frame should be arranged in descending order of population density.\n\n\n\n\n\nLet’s say that you want to describe the distribution of population densities. The following is one acceptable description (in words) that touches on shape, center, and spread of this distribution. Create a plot that fits the description, and calculate the values that should go into the blanks.\n\nThe distribution of population density of counties is unimodal and extremely right-skewed. A typical Midwestern county has population density of ____ people per unit area. The middle 50% of the counties have population densities between ___ to ___ people per unit area.\n\n\n\n\n\n\n\nTip\n\n\n\nThink about which measures of center and spread are appropriate for skewed distributions. That is, there is a right answer for the choice of functions you use to calculate the center and spread.\n\n\n\n\n\nRecall the visualization from HW 2 that showed the proportion of urban counties in each state.\n\nmidwest &lt;- midwest |&gt;\n  mutate(metro = if_else(inmetro == 1, \"Yes\", \"No\"))\n\nggplot(midwest, aes(x = state, fill = metro)) +\n  geom_bar(position = \"fill\") +\n  labs(\n    title = \"Do some states have more urban counties than others?\",\n    x = \"State\",\n    y = \"Proportion in metro area\",\n    fill = \"In metro area?\"\n  )\n\n\n\n\n\n\n\n\nIn a single data pipeline, calculate the proportion of residents in a metro area for each state.\n\n\n\nReturn to the following scatter plot of percentage below poverty vs. percentage of people with a college degree, where the color and shape of points are determined by state. Recall that you were asked to identify (by name) at least one county that is a clear outlier.\n\n\n\n\n\n\n\n\n\n\nIn a single pipeline, identify the observations marked in the orange square on the upper left corner. Your answer should be a data frame with four variables: county, state, percentage below poverty, and percentage college educated.\nIn a single pipeline, identify the observations marked in the red square in the plot above. Your answer should again be a data frame with four variables: county, state, percentage below poverty, and percentage college educated.\nBring your answers from part (a) and part (b) together! In a single pipeline, and a single filter() statement, identify the observations marked in the red and orange square in the plot above. Your answer should again be a data frame with four variables: county, state, percentage below poverty, and percentage college educated.\nCreate a new variable in midwest called potential_outlier. This variable should take on the value:\n\n\nYes if the point is one the ones you identified in part (c), i.e., one of the points marked in the squares in the plot above.\nNo otherwise.\n\nThen, display the updated midwest data frame, with county, state, percentage below poverty, percentage college educated, potential_outlier as the selected variables, arranged in ascending order of potential_outlier. (Hint: look above and/or recall how metro was calculated using the function if_else().)\n\nRecreate the visualization above, i.e. a scatterplot of percentage below poverty vs. percentage of people with a college degree, however color the points by the newly created potential_outlier variable instead of state.\n\n\n\n\n\nIn a single pipeline, calculate the total population for each state and save the resulting data frame as state_population and display it in descending order of total population.\nThen, in a separate pipeline, calculate the proportion of the total population in each state (e.g., find the percent of people living in WI out of all the people in the midwest) and, once again, display the results in descending order of proportion of population.\n\n\n\n\n\n\n\nTip\n\n\n\nIn answering parts (a) and (b), you’ll create two new variables, one for total population and other for proportion of total proportion. Make sure to give them “reasonable” names – short but evocative.\n\n\n\nWhich Midwestern state is most populous and what percent of the Midwest population lives there? Which is the least populous and what percent lives there?\n\n\n\n\nCalculate the average percentage below poverty for each state and save the resulting data frame as state_poverty with the columns state and mean_percbelowpoverty.\nThen, in a new pipeline, display the state_poverty data frame in ascending order of mean_percbelowpoverty. Which state has the lowest average percentage below poverty across its counties? Which state has the highest average percentage below poverty across its counties?"
  },
  {
    "objectID": "ds2-hw3-carlybaretz/ds2-hw3-wrangle.html#part-1",
    "href": "ds2-hw3-carlybaretz/ds2-hw3-wrangle.html#part-1",
    "title": "DS002R - HW 3 - Wrangling",
    "section": "",
    "text": "Back to the Midwest!\nIn this part you will revisit and build on some of your findings from HW 2, where you explored the midwest data frame. Remember that this data frame is bundled with the ggplot2 package and is automatically loaded when you load the tidyverse package. As a refresher, the data contains demographic characteristics of counties in the Midwest region of the United States. You can read documentation for the data set, including variable definitions by typing ?midwest in the Console or searching for midwest in the Help pane.\n\n\n\n\n\n\nNote\n\n\n\nThe data in the midwest data frame are from the 2000 census, so the information you compute below is likely slightly different from the values today.\n\n\n\n\nCalculate the number of counties in each state and display your results in descending order of number of counties. Which state has the highest number of counties, and how many? Which state has the lowest number, and how many?\n\n\n\nWithin a state, two counties can’t have the same name, but across states county names can be shared. A friend says “Look, there is a county called XYZ in EVERY state in this dataset!” In a single pipeline, discover all counties that could fill in the value of XYZ. Your output should be a data frame with two columns: county and the number of times they appear in the data.\n\n\n\n\n\n\nTip\n\n\n\nYou will want to use the filter() function in your answer, which requires a logical condition to describe what you want to filter for. For example filter(x &gt; 2) means filter for values of x greater than 2, filter(y &lt;= 3) means filter for values of y less than or equal to 3.\n\n\n\noperator\ndefinition\n\n\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to\n\n\nx & y\nx AND y\n\n\nx | y\nx OR y\n\n\nis.na(x)\ntest if x is NA\n\n\n!is.na(x)\ntest if x is not NA\n\n\nx %in% y\ntest if x is in y\n\n\n!(x %in% y)\ntest if x is not in y\n\n\n!x\nnot x\n\n\n\nThe table above is a summary of logical operators and how to articulate them in R.\n\n\n\n\n\nConsider the following box plot of population densities where you were can see some counties have extreme values.\n\n\n\n\n\n\n\n\n\nIdentify the counties described in each part:\n\nThe counties with a population density higher than 25,000. Your code must use the filter() function.\nThe county with the highest population density. Your code must use the max() function.\n\n\n\n\n\n\n\nNote\n\n\n\nAnswer using a single data wrangling pipeline for each part. Your response should be a data frame with five columns: county name, state name, population density, total population, and area, in this order. If your response has multiple rows, the data frame should be arranged in descending order of population density.\n\n\n\n\n\nLet’s say that you want to describe the distribution of population densities. The following is one acceptable description (in words) that touches on shape, center, and spread of this distribution. Create a plot that fits the description, and calculate the values that should go into the blanks.\n\nThe distribution of population density of counties is unimodal and extremely right-skewed. A typical Midwestern county has population density of ____ people per unit area. The middle 50% of the counties have population densities between ___ to ___ people per unit area.\n\n\n\n\n\n\n\nTip\n\n\n\nThink about which measures of center and spread are appropriate for skewed distributions. That is, there is a right answer for the choice of functions you use to calculate the center and spread.\n\n\n\n\n\nRecall the visualization from HW 2 that showed the proportion of urban counties in each state.\n\nmidwest &lt;- midwest |&gt;\n  mutate(metro = if_else(inmetro == 1, \"Yes\", \"No\"))\n\nggplot(midwest, aes(x = state, fill = metro)) +\n  geom_bar(position = \"fill\") +\n  labs(\n    title = \"Do some states have more urban counties than others?\",\n    x = \"State\",\n    y = \"Proportion in metro area\",\n    fill = \"In metro area?\"\n  )\n\n\n\n\n\n\n\n\nIn a single data pipeline, calculate the proportion of residents in a metro area for each state.\n\n\n\nReturn to the following scatter plot of percentage below poverty vs. percentage of people with a college degree, where the color and shape of points are determined by state. Recall that you were asked to identify (by name) at least one county that is a clear outlier.\n\n\n\n\n\n\n\n\n\n\nIn a single pipeline, identify the observations marked in the orange square on the upper left corner. Your answer should be a data frame with four variables: county, state, percentage below poverty, and percentage college educated.\nIn a single pipeline, identify the observations marked in the red square in the plot above. Your answer should again be a data frame with four variables: county, state, percentage below poverty, and percentage college educated.\nBring your answers from part (a) and part (b) together! In a single pipeline, and a single filter() statement, identify the observations marked in the red and orange square in the plot above. Your answer should again be a data frame with four variables: county, state, percentage below poverty, and percentage college educated.\nCreate a new variable in midwest called potential_outlier. This variable should take on the value:\n\n\nYes if the point is one the ones you identified in part (c), i.e., one of the points marked in the squares in the plot above.\nNo otherwise.\n\nThen, display the updated midwest data frame, with county, state, percentage below poverty, percentage college educated, potential_outlier as the selected variables, arranged in ascending order of potential_outlier. (Hint: look above and/or recall how metro was calculated using the function if_else().)\n\nRecreate the visualization above, i.e. a scatterplot of percentage below poverty vs. percentage of people with a college degree, however color the points by the newly created potential_outlier variable instead of state.\n\n\n\n\n\nIn a single pipeline, calculate the total population for each state and save the resulting data frame as state_population and display it in descending order of total population.\nThen, in a separate pipeline, calculate the proportion of the total population in each state (e.g., find the percent of people living in WI out of all the people in the midwest) and, once again, display the results in descending order of proportion of population.\n\n\n\n\n\n\n\nTip\n\n\n\nIn answering parts (a) and (b), you’ll create two new variables, one for total population and other for proportion of total proportion. Make sure to give them “reasonable” names – short but evocative.\n\n\n\nWhich Midwestern state is most populous and what percent of the Midwest population lives there? Which is the least populous and what percent lives there?\n\n\n\n\nCalculate the average percentage below poverty for each state and save the resulting data frame as state_poverty with the columns state and mean_percbelowpoverty.\nThen, in a new pipeline, display the state_poverty data frame in ascending order of mean_percbelowpoverty. Which state has the lowest average percentage below poverty across its counties? Which state has the highest average percentage below poverty across its counties?"
  },
  {
    "objectID": "ds2-hw3-carlybaretz/ds2-hw3-wrangle.html#part-2",
    "href": "ds2-hw3-carlybaretz/ds2-hw3-wrangle.html#part-2",
    "title": "DS002R - HW 3 - Wrangling",
    "section": "Part 2",
    "text": "Part 2\nPoor nameless kitties in Seattle\nUse the seattlepets dataset from the openintro R package to do some wrangling:\n\nQuestion 9\n\nHow many pets are included in the dataset? (Print the answer to the screen, show the R code which is your work, and write a complete sentence with the answer.)\nHow many variables are there for each pet? What are the names of the variables? Again, show your work using R code (not the Viewer or the Environment) and write a complete sentence.\nWhat are the three most common pet names in Seattle? (You’ll need to use the function n() which counts the number of rows.)\n\n\n\nQuestion 10\n\nWhat are the three most common names for each of the cat and dog species? Your initial code may only tell you about dogs (and the poor unnamed kitties). The slice_ family of functions pulls out a specified number of rows. For example, slice_min() pulls out the smallest rows, slice_max() pulls out the largest rows, slice_head() pulls out the first row, … (see the cheatsheets! https://www.rstudio.com/resources/cheatsheets/).\nI’ve added a new column to the dataset which gives the proportion of a particular species with the given name. Create a scatterplot of the 20 most popular pet names (as measured by the max proportion of the species with that name - the value calculated below). The x-axis will represent the proportion of cats with that name, the y-axis will represent the proportion of dogs with that name.\n\n\nseattlepets_w_prop &lt;- seattlepets |&gt;\n  group_by(species, animal_name) |&gt;\n  summarize(n_names = n()) |&gt;\n  mutate(prop_names = n_names / sum(n_names)) |&gt;\n  ungroup()\n\nhead(seattlepets_w_prop)\n\n# A tibble: 6 × 4\n  species animal_name     n_names prop_names\n  &lt;chr&gt;   &lt;chr&gt;             &lt;int&gt;      &lt;dbl&gt;\n1 Cat     \"\\\"Mama\\\" Maya\"       1  0.0000578\n2 Cat     \"\\\"Mo\\\"\"              1  0.0000578\n3 Cat     \"'Alani\"              1  0.0000578\n4 Cat     \"-\"                   1  0.0000578\n5 Cat     \"1\"                   1  0.0000578\n6 Cat     \"2\"                   1  0.0000578\n\n\nHint 1: you’ll need to pivot_ (wider or longer?)\nHint 2: after pivoting, you’ll need to sort based on the proportion. But you have two columns of proportions! Sort on the maximum of the two columns. In order to do a piece-wise maximum (element by element) in your mutate() call, use the function pmax(first column, second column, na.rm = TRUE).\nHint3: after you get the basic scatterplot made, clean it up in the following ways:\n\nadd pet name labels using geom_text() [Or better, use geom_text_repel() in ggrepel]\nadd the line y = x using geom_abline()\nmake the x-axis label something better\nremove the y-axis label and use the title to provide the y-axis (so that the letters are written horizontally instead of vertically)\nremove the poor kitties that don’t have a name"
  },
  {
    "objectID": "Project3.html",
    "href": "Project3.html",
    "title": "Mets vs Yankees Statistical Analysis",
    "section": "",
    "text": "In professional sports, salary disparities can reveal team strategies and budget allocations. This analysis focuses on the salaries of pitchers from the New York Mets and New York Yankees during the 2010 MLB season. By comparing their average salaries, I aim to understand if either team compensates its pitchers significantly more. I used the MLB dataset from the openintro package, which provides salary data for players across all MLB teams in 2010\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(openintro)\n\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\n\nlibrary(dplyr)\nknitr::opts_chunk$set(warning = FALSE, message = FALSE)\n\n\ndata(\"mlb\")\n\nFirst I filtered the dataset to only include salaries of pitchers from the Mets and the Yankees.\n\nmlb_pitchers &lt;- mlb |&gt;\n  filter((team == \"New York Mets\" | team == \"New York Yankees\") & position == \"Pitcher\" & !is.na(salary)) |&gt;\n  select(team, salary)\nmlb_pitchers\n\n# A tibble: 24 × 2\n   team          salary\n   &lt;fct&gt;          &lt;dbl&gt;\n 1 New York Mets 20145.\n 2 New York Mets 12167.\n 3 New York Mets 12000 \n 4 New York Mets  3300 \n 5 New York Mets  2900 \n 6 New York Mets  1250 \n 7 New York Mets  1250 \n 8 New York Mets  1000 \n 9 New York Mets   975 \n10 New York Mets   500 \n# ℹ 14 more rows\n\n\nThen I calculated the observed difference in average salary between the two teams.\n\n# Calculate the observed difference in average salary between Mets and Yankees\nobs_diff &lt;- mlb_pitchers |&gt;\n  group_by(team) |&gt;\n  summarize(obs_ave = mean(salary)) |&gt;\n  summarize(obs_ave_diff = diff(obs_ave)) |&gt;\n  pull(obs_ave_diff)\n\n# Check if obs_diff was calculated correctly\nobs_diff\n\n[1] 2548.385\n\n\nNext I simulated 500 permutations to determine if the observed difference is statistically significant under the null hypothesis.\nThis table, perm_stats, displays the results of 500 permutations of the salary data between Mets and Yankees pitchers, showing the calculated differences in average salary (perm_ave_diff) for each permutation. Each row represents a single permutation, providing a simulated difference under the null hypothesis to help assess the significance of the observed salary difference.\n\n# Define the permutation function to calculate permuted differences in average salary\nperm_data &lt;- function(rep, data) {\n  data |&gt;\n    mutate(salary_perm = sample(salary, replace = FALSE)) |&gt;\n    group_by(team) |&gt;\n    summarize(perm_ave = mean(salary_perm)) |&gt;\n    summarize(perm_ave_diff = diff(perm_ave), rep = rep)\n}\nset.seed(47)\n\nperm_stats &lt;- map(1:500, perm_data, data = mlb_pitchers) |&gt;\n  list_rbind()\n\nperm_stats\n\n# A tibble: 500 × 2\n   perm_ave_diff   rep\n           &lt;dbl&gt; &lt;int&gt;\n 1        -2410.     1\n 2        -3256.     2\n 3          869.     3\n 4        -2796.     4\n 5        -5145.     5\n 6         5764.     6\n 7        -1841.     7\n 8        -3057.     8\n 9          428.     9\n10        -3170.    10\n# ℹ 490 more rows\n\n\nNext I made a histogram to visualize the permuted salary differences between Mets and Yankees pitchers, with a red dashed line marking the observed difference.\n\n# Plot the permutation distribution with the observed difference\nggplot(perm_stats, aes(x = perm_ave_diff)) +\n  geom_histogram(bins = 50, fill = \"lightblue\", color = \"black\") +\n  geom_vline(xintercept = obs_diff, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Permutation Test: Difference in Mean Salary (Mets vs. Yankees Pitchers)\",\n    x = \"Difference in Permuted Mean Salary\",\n    y = \"Frequency\",\n    caption = \"Source: OpenIntro MLB Dataset\"\n  ) \n\n\n\n\n\n\n\n\nThis histogram provides descriptive statistics for the distribution of permuted mean salary differences between Mets and Yankees pitchers, showing the range and central tendency of values generated under the null hypothesis. The p-value indicates the proportion of permuted differences that exceed the observed difference, and the histogram visualizes this distribution, with a red line marking the observed difference for comparison.\nIn this analysis, I conducted a permutation test to determine if there is a significant difference in average salary between pitchers on the Mets and Yankees using the mlb dataset from the openintro package. I began by calculating the observed difference in average salary of pitchers between the two teams. Then, I defined a permutation function that randomly shuffles the salaries across teams to simulate the null hypothesis, calculating the difference in mean salary for each permutation. Finally, I repeated this process 500 times to build a distribution of permuted differences, compared the observed difference to this distribution, and computed a p-value to assess significance, visualizing the results in a histogram. This analysis examines the salaries of pitchers from the New York Mets and New York Yankees in the 2010 Major League Baseball season. The population to which these results apply is the pitchers on these two teams during this season.\nPitchers on the Mets have a higher average salary than pitchers on the Yankees by about $2,548,385. The null hypothesis is a claim about the hypothetical population in which there is no systematic difference in mean salary between these teams’ pitchers. While this dataset is limited to 2010, and results cannot be generalized across other seasons or teams without additional data, it provides insight into salary practices for pitchers on two high-profile MLB teams during this year.\nHowever, the p-value of 0.198 from the permutation test suggests that this difference isn’t statistically significant. In other words, the observed difference could reasonably occur by chance in about 20% of random permutations. So, while the Mets pitchers have a slightly higher average salary in this dataset, we don’t have strong evidence to conclude that this difference is meaningful or consistent.\n*In the dataset, salaries are represented in thousands. For example, a value of 400 corresponds to a salary of $400,000, and a value of 20144 represents $20,144,000.\nThe data used in this analysis is from the mlb dataset in the openintro R package, which provides data for educational and statistical learning purposes. The dataset contains salary information for Major League Baseball players during the 2010 season.\nThe OpenIntro team likely sourced this data from publicly available records, such as MLB salary disclosures or sports journalism databases (e.g., Baseball-Reference or USA Today’s Baseball Salary Database). These sources are generally reliable for reporting salaries in professional sports.\nHowever, it is important to note that the dataset represents a single year and does not include other forms of compensation, such as bonuses or endorsements, which could impact total player earning. https://openintrostat.github.io/openintro/reference/mlb.html"
  },
  {
    "objectID": "Project4.html",
    "href": "Project4.html",
    "title": "Analysis of Scientific Research Data",
    "section": "",
    "text": "This project explores data from the Wideband Absorbance Investigation (WAI) database to analyze the relationship between frequency and absorbance in various studies. Using SQL (Structured Query Language), I queried the database to extract and manipulate data, such as calculating average absorbance values at specific frequencies and grouping data by study identifiers.\nSQL is a powerful language used for managing and analyzing large datasets stored in relational databases. After extracting the data, I used R to visualize trends through graphs, including comparisons between male and female participants in a specific study. This project highlights how database analysis and visualization can uncover patterns in scientific research data.\n\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\nTo understand the data structure, I explored the WAI database by listing all available tables.\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\nNext I viewed the first few rows of the “Measurements” table, which allows us to understand the variables and structure of the dataset.\n\nSELECT *\nFROM Measurements\nLIMIT 0, 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931\n\n\n\n\n\nNext I only selected information about a specific study.\n\nSELECT * FROM PI_Info \nWHERE Identifier = \"Abur_2014\"; \n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nYear\nAuthors\nAuthorsShortList\nTitle\nJournal\nURL\nAbstract\nDataSubmitterName\nDataSubmitterEmail\nDateSubmitted\nPI_Notes\n\n\n\n\nAbur_2014\n2014\nDefne Abur, Nicholas J. Horton, and Susan E. Voss\nAbur et al.\nInstrasubject variability in power reflectance\nJ Am Acad Audiol\nhttps://www.ncbi.nlm.nih.gov/pubmed/25257718\n“\n\n\n\n\n\n\n\n\n Purpose:  This study investigates test-retest features of power reflectance, including comparisons of intrasubject versus intersubject variability and how ear-canal measurement location affects measurements.\n\n\n Research design:  Repeated measurements of power reflectance were made at about weekly intervals. The subjects returned for four to eight sessions. Measurements were made at three ear-canal locations: a deep insertion depth (with a foam plug flush at the entrance to the ear canal) and both 3 and 6 mm more lateral to this deep insertion.\n\n\n Study sample: Repeated measurements on seven subjects are reported. All subjects were female, between 19 and 22 yr old, and enrolled at an undergraduate women’s college.\n\n\n Data collection and analysis:  Measurements on both the right and left ears were made at three ear-canal locations during each of four to eight measurement sessions. Random-effects regression models were used for the analysis to account for repeated measures within subjects. The mean power reflectance for each position over all sessions was calculated for each subject.\n\n\n Results:  The comparison of power reflectance from the left and right ears of an individual subject varied greatly over the seven subjects; the difference between the power reflectance measured on the left and that measured on the right was compared at 248 frequencies, and depending on the subject, the percentage of tested frequencies for which the left and right ears differed significantly ranged from 10% to 93% (some with left values greater than right values and others with the opposite pattern). Although the individual subjects showed left-right differences, the overall population generally did not show significant differences between the left and right ears. The mean power reflectance for each measurement position over all sessions depended on the location of the probe in the ear for frequencies of less than 1000 Hz. The standard deviation between subjects’ mean power reflectance after controlling for ear (left or right) was found to be greater than the standard deviation within the individual subject’s mean power reflectance. The intrasubject standard deviation in power reflectance was smallest at the deepest insertion depths.\n\n\n Conclusions:  All subjects had differences in power reflectance between their left and right ears at some frequencies; the percentage of frequencies at which differences occurred varied greatly across subjects. The intrasubject standard deviations were smallest for the deepest probe insertion depths, suggesting clinical measurements should be made with as deep an insertion as practically possible to minimize variability. This deep insertion will reduce both acoustic leaks and the effect of low-frequency ear-canal losses. The within-subject standard deviations were about half the magnitude of the overall standard deviations, quantifying the extent of intrasubject versus intersubject variability.\n\n” |Susan Voss |svoss@smith.edu |24-Aug-2016 |Measurements made on 7 subjects across multiple sessions and 3 probe locations for each subject. Database includes measurements at only the deepest insertion depth (Position 1) and Channel B only. |\n\n\nNext I identified data from the “Measurements” table about a specific study.\n\nSELECT * FROM Measurements \nWHERE Identifier = \"Abur_2014\"; \n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n328.125\n0.0527801\n73326200\n-0.232837\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n351.562\n0.0583192\n68793600\n-0.232115\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n375.000\n0.0638881\n64088600\n-0.231642\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n398.438\n0.0687025\n60200600\n-0.231356\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n421.875\n0.0833181\n56990900\n-0.228356\n\n\n\n\n\nNext I selected data from the Measurements table, selecting specific Identifiers and filtering rows where Frequency is between 200 and 8000. Then I calculated the base-10 logarithm of Frequency (log_frequency) and the average absorbance (mean_absorbance), and grouped results by Identifier and Frequency.\n\nSELECT Identifier, Frequency, LOG10(Frequency) AS log_frequency,  AVG(Absorbance) AS mean_absorbance \nFROM Measurements\nWHERE Identifier IN (\"Abur_2014\", \"Feeney_2017\", \"Groon_2015\" ,\"Lewis_2015\", \"Liu_2008\"\n\"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\" , \"Sun_2016\", \"Voss_1994\", \"Voss_2010\", \"Werner_2010\" ) AND Frequency &gt; 200 AND Frequency &lt; 8000\nGROUP BY Identifier, Frequency;  \n\nThe plot visualizes how absorbance changes with frequency for given studies\n\ngraph |&gt;\nggplot(aes (x=Frequency, y = mean_absorbance, \n   color =  Identifier,\n   group = Identifier)) +\n  geom_line()+\n  scale_x_log10()+\n      labs(\n    title = \"Mean Absorbance From Each Publication in the WAI Database\",   \n    x = \"Frequency (Hz)\",           \n    y = \"Mean Absorbance\"  )\n\n\n\n\n\n\n\n\nThis table links multiple tables in the WAI database by counting distinct ears measured in the study with measurement data.\n\n SELECT p.Identifier, p.Year, p.AuthorsShortList, \n COUNT(DISTINCT SubjectNumber, Ear) AS ear_you \nFROM PI_Info AS p \nLEFT JOIN Measurements AS m ON m.Identifier = p.Identifier \nWHERE p.Identifier IN (\"Abur_2014\", \"Feeney_2017\", \"Groon_2015\" ,\"Lewis_2015\", \"Liu_2008\"\n\"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\" , \"Sun_2016\", \"Voss_1994\", \"Voss_2010\", \"Werner_2010\" ) AND Frequency &gt; 200 AND Frequency &lt; 8000\nGROUP BY Identifier, Instrument;\n\n\nDisplaying records 1 - 10\n\n\nIdentifier\nYear\nAuthorsShortList\near_you\n\n\n\n\nAbur_2014\n2014\nAbur et al.\n14\n\n\nFeeney_2017\n2017\nFeeney et al.\n57\n\n\nGroon_2015\n2015\nGroon et al.\n21\n\n\nLewis_2015\n2015\nLewis and Neely\n14\n\n\nShahnaz_2006\n2006\nShahnaz and Bork\n237\n\n\nShaver_2013\n2013\nShaver and Sun\n48\n\n\nSun_2016\n2016\nSun\n84\n\n\nVoss_1994\n1994\nVoss and Allen\n10\n\n\nVoss_2010\n2010\nVoss et al.\n12\n\n\nWerner_2010\n2010\nWerner et al.\n962\n\n\n\n\n\nThis creates 1 table to combine data from different tables.\n\n SELECT p.Identifier, Year, AuthorsShortList, \n COUNT(DISTINCT SubjectNumber, Ear) AS ear_you,\n CONCAT(AuthorsShortList, \" (\" , year, \") \" , \"N=\" , \n COUNT(DISTINCT SubjectNumber, Ear), \"; \", Instrument) AS legend \nFROM PI_Info AS p \nLEFT JOIN Measurements AS m ON m.Identifier = p.Identifier \nWHERE p.Identifier IN (\"Abur_2014\", \"Feeney_2017\", \"Groon_2015\" ,\"Lewis_2015\", \"Liu_2008\"\n\"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\" , \"Sun_2016\", \"Voss_1994\", \"Voss_2010\", \"Werner_2010\" ) AND Frequency &gt; 200 AND Frequency &lt; 8000\nGROUP BY Identifier, Instrument;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\nIdentifier\nYear\nAuthorsShortList\near_you\nlegend\n\n\n\n\nAbur_2014\n2014\nAbur et al.\n14\nAbur et al. (2014) N=14; HearID\n\n\nFeeney_2017\n2017\nFeeney et al.\n57\nFeeney et al. (2017) N=57; preTitan\n\n\nGroon_2015\n2015\nGroon et al.\n21\nGroon et al. (2015) N=21; Other\n\n\nLewis_2015\n2015\nLewis and Neely\n14\nLewis and Neely (2015) N=14; Other\n\n\nShahnaz_2006\n2006\nShahnaz and Bork\n237\nShahnaz and Bork (2006) N=237; HearID\n\n\nShaver_2013\n2013\nShaver and Sun\n48\nShaver and Sun (2013) N=48; preTitan\n\n\nSun_2016\n2016\nSun\n84\nSun (2016) N=84; preTitan\n\n\nVoss_1994\n1994\nVoss and Allen\n10\nVoss and Allen (1994) N=10; preHearID\n\n\nVoss_2010\n2010\nVoss et al.\n12\nVoss et al. (2010) N=12; HearID\n\n\nWerner_2010\n2010\nWerner et al.\n962\nWerner et al. (2010) N=962; Other\n\n\n\n\n\nThis adds on to the table above, including mean absorbance across frequencies.\n\n SELECT p.Identifier, p.Year, p.AuthorsShortList, Frequency,\n LOG10(Frequency) AS log_frequency, AVG(Absorbance) AS mean_absorbance,\n COUNT(DISTINCT SubjectNumber, Ear) AS ear_you,\n CONCAT(AuthorsShortList, \" (\" , year, \") \", \"N=\", COUNT(DISTINCT SubjectNumber, Ear), \"; \", Instrument) AS legend\nFROM PI_Info AS p \nLEFT JOIN Measurements AS m ON m.Identifier = p.Identifier \nWHERE p.Identifier IN (\"Abur_2014\", \"Feeney_2017\", \"Groon_2015\" ,\"Lewis_2015\", \"Liu_2008\"\n\"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\" , \"Sun_2016\", \"Voss_1994\", \"Voss_2010\", \"Werner_2010\" ) AND Frequency &gt; 200 AND Frequency &lt; 8000\nGROUP BY Identifier, Instrument, Frequency;\n\n\ngraph2 |&gt;\nggplot(aes (x=Frequency, y = mean_absorbance, \n   color =  legend,\n   group = legend)) +\n  geom_line()+\n  scale_x_log10()+\n        labs(\n    title = \"Mean Absorbance From Each Publication in the WAI Database\",\n    legend = \"\",\n    x = \"Frequency (Hz)\",           \n    y = \"Mean Absorbance\",\n    color = NULL)\n\n\n\n\n\n\n\n\nThis looks at all the information in the “Subjects” table.\n\nSELECT * \nFROM Subjects\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSessionTotal\nAgeFirstMeasurement\nAgeCategoryFirstMeasurement\nSex\nRace\nEthnicity\nLeftEarStatusFirstMeasurement\nRightEarStatusFirstMeasurement\nSubjectNotes\n\n\n\n\nAbur_2014\n1\n7\n20.0000000\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n3\n8\n19.0000000\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\nSession 5 not included do to acoustic leak\n\n\nAbur_2014\n4\n7\n21.0000000\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n6\n8\n21.0000000\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n7\n5\n20.0000000\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n8\n5\n19.0000000\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n10\n5\n19.0000000\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\nSession 4 not included do to acoustic leak\n\n\nAithal_2013\n1\n1\nNA\nInfant\nMale\nUnknown\nUnknown\nNormal\nUnknown\nNA\n\n\nAithal_2013\n2\n1\n0.0074418\nInfant\nFemale\nUnknown\nUnknown\nNormal\nUnknown\nNA\n\n\nAithal_2013\n3\n1\nNA\nInfant\nMale\nUnknown\nUnknown\nUnknown\nNormal\nNA\n\n\n\n\n\nThis calculates mean absorbance for males and females separately in the Aithal 2013 study.\n\nSELECT Sex, Frequency, AVG(Absorbance) AS mean_absorbance, m.Identifier \nFROM Subjects AS s \nRIGHT JOIN Measurements AS m ON m.SubjectNumber = s.SubjectNumber\nWHERE m.Identifier = \"Aithal_2013\" AND Frequency &gt; 200 AND Frequency &lt; 8000\nGROUP BY m.Identifier, Frequency, Sex;\n\n\nhead(graphforQ2)\n\n      Sex Frequency mean_absorbance  Identifier\n1  Female       250       0.5436131 Aithal_2013\n2    Male       250       0.5444056 Aithal_2013\n3 Unknown       250       0.6231640 Aithal_2013\n4  Female       300       0.5163662 Aithal_2013\n5    Male       300       0.5221938 Aithal_2013\n6 Unknown       300       0.5825467 Aithal_2013\n\n\nThis visualizes absorbance trends by gender in the Aithal 2013 study.\n\ngraphforQ2 |&gt;\n  ggplot(aes(\n    x = Frequency,\n    y = mean_absorbance,\n    color = Sex,\n    group = Sex\n  )) +\n  geom_line() +\n  scale_x_log10() +\n  labs(\n    title = \"Mean Absorbance Across Frequencies by Gender in 'Aithal_2013' Study\", \n    x = \"Frequency\",\n    y = \"Mean Absorbance\", \n    color = \"Gender\"  \n  ) +\n  facet_wrap(~Sex)  \n\n\n\n\n\n\n\n\nIn this project, I analyzed data from the WAI database to explore the relationship between frequency and absorbance across various studies. I used SQL queries to extract, filter, and calculate the average absorbance values for specific frequencies and grouped the data by study identifiers. These results were then visualized in R using ggplot, with plots on a logarithmic frequency scale to highlight trends. Additionally, I compared absorbance data by sex for a specific study to observe any potential differences, finding that male and female values were the same.\nI got my data from the WAI Database and the graph is modeled from a graph in this study: Voss, Susan E. Ph.D. Resource Review. Ear and Hearing 40(6):p 1481, November/December 2019. | DOI: 10.1097/AUD.0000000000000790"
  },
  {
    "objectID": "presentation.html#slide-2",
    "href": "presentation.html#slide-2",
    "title": "Carly Presentation",
    "section": "slide 2",
    "text": "slide 2"
  },
  {
    "objectID": "presentation.html#test",
    "href": "presentation.html#test",
    "title": "Carly’s Final Data Science Presentation",
    "section": "test",
    "text": "test\nis this gonna work??"
  },
  {
    "objectID": "presentation.html#test-2",
    "href": "presentation.html#test-2",
    "title": "Carly’s Final Data Science Presentation",
    "section": "test 2",
    "text": "test 2"
  },
  {
    "objectID": "presentation.html#salary-comparison-of-pitchers-new-york-mets-vs.-new-york-yankees-2010",
    "href": "presentation.html#salary-comparison-of-pitchers-new-york-mets-vs.-new-york-yankees-2010",
    "title": "Carly’s Final Data Science Presentation",
    "section": "Salary Comparison of Pitchers: New York Mets vs. New York Yankees (2010)",
    "text": "Salary Comparison of Pitchers: New York Mets vs. New York Yankees (2010)\nThis analysis investigates salary differences between pitchers from the New York Mets and New York Yankees in the 2010 MLB season\nMLB data from the openintro package (2010 season)"
  },
  {
    "objectID": "presentation.html#motivation",
    "href": "presentation.html#motivation",
    "title": "Carly’s Final Data Science Presentation",
    "section": "Motivation",
    "text": "Motivation"
  },
  {
    "objectID": "presentation.html#mlb-salaries",
    "href": "presentation.html#mlb-salaries",
    "title": "Carly’s Final Data Science Presentation",
    "section": "MLB Salaries",
    "text": "MLB Salaries"
  },
  {
    "objectID": "presentation.html#mets-and-yankees-pitcher-salaries",
    "href": "presentation.html#mets-and-yankees-pitcher-salaries",
    "title": "Carly’s Final Data Science Presentation",
    "section": "Mets and Yankees Pitcher Salaries",
    "text": "Mets and Yankees Pitcher Salaries\n\nmlb_pitchers &lt;- mlb |&gt;\n  filter((team == \"New York Mets\" | team == \"New York Yankees\") & position == \"Pitcher\" & !is.na(salary)) |&gt;\n  select(team, salary)\nmlb_pitchers\n\n# A tibble: 24 × 2\n   team          salary\n   &lt;fct&gt;          &lt;dbl&gt;\n 1 New York Mets 20145.\n 2 New York Mets 12167.\n 3 New York Mets 12000 \n 4 New York Mets  3300 \n 5 New York Mets  2900 \n 6 New York Mets  1250 \n 7 New York Mets  1250 \n 8 New York Mets  1000 \n 9 New York Mets   975 \n10 New York Mets   500 \n# ℹ 14 more rows\n\n\n* Salaries are represented in the thousands, so 20,144 is actually $20,144,000."
  },
  {
    "objectID": "presentation.html#observed-difference",
    "href": "presentation.html#observed-difference",
    "title": "Carly’s Final Data Science Presentation",
    "section": "Observed Difference",
    "text": "Observed Difference\n\n# Calculate the observed difference in average salary between Mets and Yankees\nobs_diff &lt;- mlb_pitchers |&gt;\n  group_by(team) |&gt;\n  summarize(obs_ave = mean(salary)) |&gt;\n  summarize(obs_ave_diff = diff(obs_ave)) |&gt;\n  pull(obs_ave_diff)\n\n# Check if obs_diff was calculated correctly\nobs_diff\n\n[1] 2548.385"
  },
  {
    "objectID": "presentation.html#permutation-test",
    "href": "presentation.html#permutation-test",
    "title": "Carly’s Final Data Science Presentation",
    "section": "Permutation Test",
    "text": "Permutation Test\n\nperm_data &lt;- function(rep, data) {\n  data |&gt;\n    mutate(salary_perm = sample(salary, replace = FALSE)) |&gt;\n    group_by(team) |&gt;\n    summarize(perm_ave = mean(salary_perm)) |&gt;\n    summarize(perm_ave_diff = diff(perm_ave), rep = rep)\n}\nset.seed(47)\n\nperm_stats &lt;- map(1:500, perm_data, data = mlb_pitchers) |&gt;\n  list_rbind()\n\nperm_stats\n\n# A tibble: 500 × 2\n   perm_ave_diff   rep\n           &lt;dbl&gt; &lt;int&gt;\n 1        -2410.     1\n 2        -3256.     2\n 3          869.     3\n 4        -2796.     4\n 5        -5145.     5\n 6         5764.     6\n 7        -1841.     7\n 8        -3057.     8\n 9          428.     9\n10        -3170.    10\n# ℹ 490 more rows"
  },
  {
    "objectID": "presentation.html#visualization",
    "href": "presentation.html#visualization",
    "title": "Carly’s Final Data Science Presentation",
    "section": "Visualization",
    "text": "Visualization"
  },
  {
    "objectID": "presentation.html#conclusion",
    "href": "presentation.html#conclusion",
    "title": "Carly’s Final Data Science Presentation",
    "section": "Conclusion",
    "text": "Conclusion\nPitchers on the Mets earned a higher average salary than Yankees pitchers in 2010.\nHOWEVER, the difference is not statistically significant."
  }
]